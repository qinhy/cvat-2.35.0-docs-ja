
# 52.自動アノテーション

CVATの自動アノテーションは、事前学習済みモデルを使ってデータに自動的に事前アノテーションを付与できるツールです。

CVATは以下のソースからモデルを利用できます：

- [プリインストール済みモデル](#models)。
- [Hugging Face および Roboflow](#adding-models-from-hugging-face-and-roboflow) から統合したモデル。
- Nuclioでデプロイしたセルフホストモデル。

以下の表は利用可能なオプションについて説明しています：

|                                             | セルフホスト            | クラウド                                            |
| ------------------------------------------- | ---------------------- | ------------------------------------------------ |
| **価格**                                   | 無料                   | [価格表](https://www.cvat.ai/pricing/cloud) を参照 |
| **モデル**                                 | モデルを追加する必要あり | プリインストール済みモデルを利用可能                 |
| **Hugging Face & Roboflow <br>統合**       | 非対応                 | 対応                                               |

参照：

- [自動アノテーションの実行](#running-automatic-annotation)
- [ラベルのマッチング](#labels-matching)
- [モデル一覧](#models)
- [Hugging Face および Roboflow からのモデル追加](#adding-models-from-hugging-face-and-roboflow)

## 自動アノテーションの実行

自動アノテーションを開始するには、以下の手順を実行してください：

1. 上部メニューから **Tasks** をクリックします。
1. アノテーションしたいタスクを探し、**Action** > **Automatic annotation** をクリックします。

   ![](./images/image119_detrac.jpg)

1. 自動アノテーションのダイアログで、ドロップダウンリストから[モデル](#models)を選択します。
1. モデルとタスクの[ラベルをマッチング](#labels-matching)します。
1. （任意）マスクをポリゴンとして返したい場合は、**Return masks as polygons** のトグルをオンにします。
1. （任意）過去のアノテーションをすべて削除したい場合は、**Clean old annotations** のトグルをオンにします。
1. （任意）モデルの**閾値**を指定できます。指定しない場合は、モデル設定のデフォルト値が使用されます。

   ![](./images/running_automatic_annotation.png)

1. **Annotate** をクリックします。

CVATはプログレスバーでアノテーションの進捗を表示します。

![プログレスバー](./images/image121_detrac.jpg)

自動アノテーションは、いつでもキャンセルをクリックすることで停止できます。

## ラベルのマッチング

各モデルは特定のデータセットで学習されており、そのデータセットのラベルのみ対応しています。

例：

- DLモデルには `car` というラベルがある。
- あなたのタスク（またはプロジェクト）には `vehicle` というラベルがある。

アノテーションするには、これら2つのラベルをマッチさせて、
この場合 `car` = `vehicle` であることをCVATに伝える必要があります。

もしDLラベル一覧に存在しないラベルがある場合、
それらをマッチさせることはできません。

このため、対応しているDLモデルは特定のラベルにのみ利用可能です。

各モデルのラベル一覧を確認するには、[モデル一覧](#models)の論文や公式ドキュメントを参照してください。

## モデル一覧

自動アノテーションはプリインストール済みモデルと追加したモデルを利用します。

> セルフホスト型の場合は、
> まず
> 自動アノテーションをインストール
> し、モデルを追加する必要があります。

プリインストール済みモデル一覧：

| モデル | 説明 |
|--------|------|
| 属性付き顔検出 | 3つのOpenVINOモデルを組み合わせて使用:<br><br>- [Face Detection 0205](https://docs.openvino.ai/2022.3/omz_models_model_face_detection_0205.html): MobileNetV2ベースのFCOSヘッドを使用した顔検出器。屋内外の正面向きカメラ映像に対応。<br>- [Emotions recognition retail 0003](https://docs.openvino.ai/2022.3/omz_models_model_emotions_recognition_retail_0003.html): 5つの感情(neutral、happy、sad、surprise、anger)を認識する畳み込みネットワーク。<br>- [Age gender recognition retail 0013](https://docs.openvino.ai/2022.3/omz_models_model_age_gender_recognition_retail_0013.html): 年齢(18-75歳)と性別を同時に認識する畳み込みネットワーク。 |
| RetinaNet R101 | クラス不均衡に対応するフォーカルロス関数を使用した1ステージ物体検出モデル。バックボーンネットワークと2つのタスク特化型サブネットワークで構成。<br><br>詳細: [RetinaNET](https://paperswithcode.com/lib/detectron2/retinanet) |
| テキスト検出 | MobileNetV2(depth_multiplier=1.4)をバックボーンとしたPixelLinkベースのテキスト検出器。屋内外シーンに対応。<br><br>詳細: [OpenVINO Text detection 004](https://docs.openvino.ai/2022.3/omz_models_model_text_detection_0004.html) |
| YOLO v3 | COCOデータセットで事前学習された物体検出モデル群。<br><br>詳細: [YOLO v3](https://docs.openvino.ai/2022.3/omz_models_model_yolo_v3_tf.html) |
| YOLO v7 | 高速・高精度な最新の物体検出モデル。5-160FPSのフレームレートに対応し、30FPS以上のリアルタイム検出器中で最高の56.8% APを達成。<br><br>詳細:<br>- [GitHub](https://github.com/WongKinYiu/yolov7)<br>- [論文](https://arxiv.org/pdf/2207.02696.pdf) |
## Hugging Face および Roboflow からのモデル追加

必要なモデルが見つからない場合は、[Hugging Face](https://huggingface.co/) または [Roboflow](https://roboflow.com/) からお好きなモデルを追加できます。

> **注意**：セルフホストCVATにはHugging FaceおよびRoboflowからモデルを追加できません。

<!--lint disable maximum-line-length-->

詳細は
[Hugging FaceおよびRoboflowモデル統合によるアノテーション効率化](https://www.cvat.ai/post/integrating-hugging-face-and-roboflow-models)
を参照してください。

以下のビデオでプロセスを説明しています：

[![YouTube Video](https://img.youtube.com/vi/SbU3aB65W5s/0.jpg)](https://www.youtube.com/watch?v=SbU3aB65W5s)

<!--lint enable maximum-line-length-->